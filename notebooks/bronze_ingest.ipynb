{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "id": "e859418c-ea8c-4d7b-b292-3df9b3104edc",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_1"
      },
      "source": "%%sql -r dataframe_1\nUSE DATABASE DBT_DEV_DB;\nUSE SCHEMA BRONZELAYER;\n\nSELECT current_database() AS DATABASE_NAME,\n current_schema() AS SCHEMA_NAME;",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "69717be5-d7ff-40a5-be51-a6d1f2e4c6b1",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import logging\nimport pandas as pd\nfrom datetime import datetime\nfrom snowflake.snowpark.context import get_active_session\n\nlogger = logging.getLogger(\"bronze_ingest\")\nlogger.setLevel(logging.INFO)\n\nsession = get_active_session()\n\n# Get context directly from Snowflake\ndb = session.get_current_database()\nschema = session.get_current_schema()\n\nprint(f\"Running in: {db}.{schema}\")\nlogger.info(f\"Bronze ingestion started in {db}.{schema}\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d6054f50-f3ad-45c5-b4a7-6cc4d7c9bc72",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# =========================================\n# Multi-CSV Bronze Ingestion (Fully Fixed)\n# =========================================\n\nfrom datetime import datetime\nimport pandas as pd\nimport os\n\nSTAGE_NAME = f\"@{db}.{schema}.DEV_RAW_FILES_STAGE\"\n\n# Get stage file list\nfiles_df = session.sql(f\"LIST {STAGE_NAME}\").to_pandas()\n\n# Clean weird Snowflake quoted column names\nfiles_df.columns = [c.replace('\"', '').upper() for c in files_df.columns]\n\nprint(\"Cleaned columns:\", files_df.columns)\n\n# Extract CSV files\ncsv_files = [\n    row[\"NAME\"].split(\"/\")[-1]\n    for _, row in files_df.iterrows()\n    if row[\"NAME\"].lower().endswith(\".csv\")\n]\n\nprint(f\"Found {len(csv_files)} CSV files.\")\n\nfor FILE_NAME in csv_files:\n\n    TABLE_NAME = \"RAW_\" + os.path.splitext(FILE_NAME)[0].upper()\n\n    print(f\"\\nProcessing: {FILE_NAME} â†’ {TABLE_NAME}\")\n\n    session.file.get(\n        f\"{STAGE_NAME}/{FILE_NAME}\",\n        \"/tmp/\"\n    )\n\n    df = pd.read_csv(f\"/tmp/{FILE_NAME}\")\n\n    df.columns = [\n        col.strip().upper().replace(\" \", \"_\").replace(\"-\", \"_\")\n        for col in df.columns\n    ]\n\n    df[\"_LOADED_AT\"] = datetime.utcnow()\n    df[\"_SOURCE_FILE\"] = FILE_NAME\n\n    snow_df = session.create_dataframe(df)\n    snow_df.write.mode(\"append\").save_as_table(f\"{db}.{schema}.{TABLE_NAME}\")\n\n    row_count = session.table(f\"{db}.{schema}.{TABLE_NAME}\").count()\n\n    print(f\"Loaded {len(df)} rows. Table now has {row_count} rows.\")\n\nprint(\"\\nAll CSV files processed successfully.\")\n",
      "outputs": [],
      "execution_count": null
    }
  ]
}